---
title: "results"
author: "Eli Metzner"
date: "1/5/2020"
header-includes:
    - \setlength{\parindent}{1.25cm}
    - \usepackage{setspace}\doublespacing
    - \AtBeginDocument{\let\maketitle\relax}
    - \usepackage{amsmath}
    - \usepackage{indentfirst}
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tseries)
library(forecast)
library(readxl)
library(ggfortify)
library(glmnet)
library(stargazer)
library(keras)
setwd("~/Dropbox/Documents/yale/senior/ECON_491_492/data/final datasets")
```

# Thesis Update and Results - 1/2020

### Kiley dataset

Recall the Kiley dataset originally had 118 observations of 26 financial stability indicators measured quarterly from Q1 1990 to Q2 2019. After diff'ing to assure stationarity, we're left with 117 differenced observations representing the quarter-on-quarter change for each indicator. We first perform PCA, which should give an idea of the indicators that contribute most to the variance within the dataset.

```{r echo = FALSE}
setwd("~/Dropbox/Documents/yale/senior/ECON_491_492/data/final datasets")
kiley = read_excel("kiley_labeled.xlsx") %>%
  select_at(3:29) %>%
  mutate_all(as.numeric) 

# test and assure stationarity, scale to zero mean and unit variance
trends = sapply(kiley, ndiffs, alpha = 0.05, test = "adf")
kiley_stationary = data.frame(sapply(kiley, diff)) %>%
  select_at(-1) %>%
  mutate_all(scale)

### PCA
kileyPCA = prcomp(kiley_stationary)
dfPCA = as_tibble(kileyPCA$rotation) %>%
  mutate(feature = rownames(kileyPCA$rotation)) %>%
  arrange(-PC1) %>%
  select_at(c(1:3, 27)) %>%
  pivot_longer(1:3, names_to = 'component', values_to = 'loading')

# heatmap of loadings
ggplot(dfPCA) +
  geom_tile(aes(x = feature, y = component, fill = loading)) +
  scale_fill_distiller(palette = "RdBu") +
  coord_equal() +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 9))
```

The heatmap is interesting -- looks like the first PC (which explains ~20% of the variance) is highly correlated with asset pricing variables, and the second two have high loadings on the financial sector variables.

We now introduce our two outcome variables, the TED spread and the FSS index. The TED spread is a measure of bank funding liquidity that quantifies the cost of interbank lending, and we hypothesize can be predicted by strain on the financial system, the idea being that stress to the financial system in period t-1 causes decreased liquidity and therefore an elevated TED spread in period t. If this is the case -- i.e., if the information in the Kiley dataset is accurately explained by the first two PCs, and the variance explained by the first two PCs corresponds to variance in the TED spread, we expect to see a clear pattern in the scatterplot here. This would in turn hint that Kiley's indicators are predictive of the TED spread.

```{r echo=FALSE}
outcome_TED = log(kiley$outcome_TEDspread[2:118])
df_outcome = cbind(kileyPCA$x, outcome_TED)
ggplot(df_outcome) +
  geom_point(aes(x = PC1, y = PC2, color = outcome_TED)) +
  scale_color_viridis_c() +
  labs(x = "PC1 (18.79%)", y = "PC2 (14.79%)") +
  theme_minimal()
```

This is promising, as it looks like the log TED spread tends to increase with the first PC. We now introduce the FSS index, a measure of the quarterly "financial stability sentiment" of the FOMC generated by applying natural language processing techniques to the March, June/July, September, and December FOMC minutes using a dictionary and scoring system described by Londono et al. Per Londono et al, positive FSS scores indicates deteriorating Fed sentiment as to the health of the financial system. We now color the scatterplot of the first two PCs in Kiley by the scaled FSS score in the next period.

```{r echo = FALSE}
kileyF = read_excel("kiley_labeled_FSS.xlsx") %>%
  select_at(3:29) %>%
  mutate_all(as.numeric) 

outcome_FSS = scale(kileyF$outcome_FSS[2:118])
df_outcome = cbind(kileyPCA$x, outcome_FSS)
ggplot(df_outcome) +
  geom_point(aes(x = PC1, y = PC2, color = outcome_FSS)) +
  scale_color_viridis_c() +
  labs(x = "PC1 (18.79%)", y = "PC2 (14.79%)") +
  theme_minimal()
```

This plot is less clear, though we might see some variation in the FSS score with PC2. Either way, it looks relatively promising to continue estimating the models.

### Kiley - TED spread

Now we'd like to quantify the effect of each of Kiley's indicators on liquidity and the Fed's financial sentiment. We have two goals: first, we obviously want to accurately predict liquidity in the period ahead given the state of the financial system in the current period; second, we want this predictive model to be interpretable, i.e. we want the model to identify which of the 26 features in Kiley's dataset carry the most predictive power. The traditional regression framework cannot be accurately applied here as our dataset is high-dimensional and highly correlated, and the TED spread is itself a (stationary) time series with an autoregressive component. Therefore, we estimate an autoregressive LASSO model, regularizing the coefficients in order to perform inline feature selection in addition to getting the best predictions. We start with the model for the TED spread, specified as follows:

$$
\log(y_t) = \mu + \phi_1 \log(y_{t-1}) + \Lambda \boldsymbol{X_{t-1}} + \epsilon_t
$$

where we're modeling the log of the TED spread $y_t$ in period $t$ given the log TED spread in period $t-1$ and a vector of differenced indicators $\boldsymbol{X_{t-1}}$ reflecting the change in each series in Kiley's dataset from period $t-2$ to period $t-1$.  We choose a training set of observations from Q3 1990 to Q4 2012 (the first 90 observations we can use, after differencing) and test on Q1 2013 to Q2 2019 (the last 26) -- this corresponds to about an 80/20% train/test split. To get an idea of the LASSO model's performance, we first fit an ARIMA baseline with the `auto.arima` function to ensure we get the best time series model for comparison, and then we score the models on the mean absolute error of the log TED spread on the test set.

```{r echo = FALSE, warning = FALSE}
.lagTED = log(kiley$outcome_TEDspread)[2:91]
x2 = data.matrix(cbind(kiley_stationary[1:90,], .lagTED))
outcome_TED = log(kiley$outcome_TEDspread)[3:92]
x2_test = data.matrix(cbind(kiley_stationary[91:116,], log(kiley$outcome_TEDspread)[92:117]))
y2_test = log(kiley$outcome_TEDspread)[93:118]

model_arima = auto.arima(outcome_TED)
pred_ar = predict(model_arima, n.ahead = 26)
mae_ar = mean(abs(y2_test - pred_ar$pred))

seq_lambda = seq(0.1, 0, by = -0.001)
mae_train = vector(mode = 'numeric', length = 100)
mae_test = vector(mode = 'numeric', length = 100)
lasso1 = glmnet(x2, outcome_TED, type.measure = "mse", alpha = 1, lambda = seq_lambda)
for (lambda in seq_lambda) {
  pred_train = predict(lasso1, s = lambda, newx = x2)
  pred_test = predict(lasso1, s = lambda, newx = x2_test)
  mae_train[which(seq_lambda == lambda)] = mean(abs(pred_train - outcome_TED))
  mae_test[which(seq_lambda == lambda)] = mean(abs(pred_test - y2_test))
}

### produce model results
# train and test error per lambda
df = data.frame('lambda' = seq_lambda, mae_train, mae_test) %>%
  pivot_longer(2:3, names_to = "series", values_to = "mae")
ggplot(df) +
  geom_line(aes(x = lambda, y = mae, color = series)) +
  theme_minimal()

# predictions
df_pred = data.frame(predict(lasso1, s = seq_lambda[which.min(mae_test)], newx = x2_test),
                     as.double(pred_ar$pred), y2_test) %>%
  mutate(time = seq(2013.25, 2019.5, by = 0.25))
colnames(df_pred) = c('lasso', 'AR_1', 'TED', 'time')
df_pred = df_pred %>%
  pivot_longer(1:3, names_to = 'series', values_to = 'value')
ggplot(df_pred) +
  geom_line(aes(x = time, y = value, color = series)) +
  theme_minimal()

```

```{r results = 'asis', echo = FALSE}
df_coef = enframe(data.matrix(coef(lasso1))[,which.min(mae_test)]) %>%
  arrange(-abs(value)) %>%
  mutate(value = round(value, digits = 3)) %>%
  rename(Coefficient = value) %>%
  separate(name, into = c("Class", "Indicator"), fill = "left")
stargazer(df_coef, summary = FALSE, rownames = FALSE, header = FALSE)

```

```{r}
# LASSO model R2
(r2 = lasso1$dev.ratio[which.min(mae_test)])

# pct improvement over AR(1), log bp
(pct_improvement_log = 100* (mae_ar - min(mae_test)) / mae_ar)
```

The LASSO model provides good fit at a reasonable $\lambda$ value (shown in the train/test error plot over 100 values), selecting 13 nonzero features and the lagged log TED spread. The predictions appear reasonable and we see clear improvement in predictive performance over the AR(1) baseline. Good start. Will leave the interpretation of the coefficients for a separate section.

### Kiley - FSS index

Next, we want to see if the same indicators are predictive of the Fed's financial stability sentiment -- if this is the case, we can have confidence that the lasso model selected valuable features that inform financial system strain as measured in two completely separate ways. We therefore specify the FSS model as follows:

$$
s_t = \nu + \gamma_1 s_{t-1} + \Theta \boldsymbol{X_{t-1}} + \epsilon_t
$$

where, similarly, we're modeling the (scaled to zero mean/unit variance) FSS score $s_t$ in period $t$ given the scaled score in period $t-1$ and the same vector of differenced indicators $\boldsymbol{X_{t-1}}$ reflecting the change in each series in Kiley's dataset from period $t-2$ to period $t-1$. We fit the LASSO by the same procedure and produce the same model diagnostics:

```{r echo = FALSE, warning = FALSE}
kiley = read_excel("kiley_labeled_FSS.xlsx") %>%
  select_at(3:29) %>%
  mutate_all(as.numeric) 

kiley_stationary = data.frame(sapply(kiley, diff)) %>%
  select_at(-1) %>%
  mutate_all(scale)

.lagFSS = scale(kiley$outcome_FSS)[2:91]
x2 = data.matrix(cbind(kiley_stationary[1:90,], .lagFSS))
outcome_FSS = scale(kiley$outcome_FSS)[3:92]
x2_test = data.matrix(cbind(kiley_stationary[91:116,], scale(kiley$outcome_FSS)[92:117]))
y2_test = scale(kiley$outcome_FSS)[93:118]

model_arima = auto.arima(outcome_FSS)
pred_ar = predict(model_arima, n.ahead = 26)
mae_ar = mean(abs(y2_test - pred_ar$pred))

seq_lambda = seq(0.2, 0, by = -0.001)
mae_train = vector(mode = 'numeric', length = 200)
mae_test = vector(mode = 'numeric', length = 200)
lasso1 = glmnet(x2, outcome_FSS, type.measure = "mse", alpha = 1, lambda = seq_lambda)
for (lambda in seq_lambda) {
  pred_train = predict(lasso1, s = lambda, newx = x2)
  pred_test = predict(lasso1, s = lambda, newx = x2_test)
  mae_train[which(seq_lambda == lambda)] = mean(abs(pred_train - outcome_FSS))
  mae_test[which(seq_lambda == lambda)] = mean(abs(pred_test - y2_test))
}

### produce model results
# train and test error per lambda
df = data.frame('lambda' = seq_lambda, mae_train, mae_test) %>%
  pivot_longer(2:3, names_to = "series", values_to = "mae")
ggplot(df) +
  geom_line(aes(x = lambda, y = mae, color = series)) +
  theme_minimal()

# r2 and pct improvement over ar(1)
df_coef = enframe(data.matrix(coef(lasso1))[,which.min(mae_test)]) %>%
  arrange(-abs(value)) %>%
  mutate(value = round(value, digits = 3)) %>%
  rename(Coefficient = value) %>%
  separate(name, into = c("Class", "Indicator"), fill = "left")

# predictions
df_pred = data.frame(predict(lasso1, s = seq_lambda[which.min(mae_test)], newx = x2_test),
                     as.double(pred_ar$pred), y2_test) %>%
  mutate(time = seq(2013.25, 2019.5, by = 0.25))
colnames(df_pred) = c('lasso', 'ARMA', 'FSS', 'time')
df_pred = df_pred %>%
  pivot_longer(1:3, names_to = 'series', values_to = 'value')
ggplot(df_pred) +
  geom_line(aes(x = time, y = value, color = series)) +
  theme_minimal()
```

```{r results = 'asis'}
stargazer(df_coef, summary = FALSE, rownames = FALSE, header = FALSE)
```

```{r}
# FSS model R2
(r2 = lasso1$dev.ratio[which.min(mae_test)])

# FSS model pct improvement over ARMA(1,1)
(pct_improvement = 100* (mae_ar - min(mae_test)) / mae_ar)
```

This is where things start breaking down. Our model fit clearly decreases compared to the TED spread, which we can notice immediately in two ways: first, the $R^2$ is only 0.27, which while not at the level of uninterpretable certainly doesn't give much confidence in the model results. I think we have a very good visualization of the bias-variance tradeoff here, in kind of an unfortunate way, because as we increase $\lambda$ in order to prevent overfitting and reduce the variance of the estimator, the model's bias increases such that we lose overall fit. Basically, if we chose a smaller $\lambda$ as in the last model, we'd have a higher $R^2$, so we could consider optimizing the regularization based on that instead of test error (although the $R^2$ increases monotonically as we add more variables to the model, meaning it strictly decreases as we increase $\lambda$, so it might not actually be a meaningful parameter to score the model on). Note that we can't compare the test MAE in the TED spread model to this one, as the units are different, but the percent improvement should provide a consistent basis for comparison. Accordingly, we only get 17% improvement over the ARIMA baseline here, and the selected features don't seem to correlate well to the ones selected for the TED spread model. Again leaving interpretation to later, but these results don't seem ideal.

### Aikman - LIBOR-Repo spread

As a robustness check, we now introduce an analog of the first model from a different economy. Aikman et al. have collected a dataset of financial stability indicators for the UK, which though more sparse than Kiley's in terms of both features and observations, still contains complete data for 90 quarterly observations of 16 featuers from Q1 1996 to Q2 2019. For our outcome variable, we use the UK analog of the TED spread, the spread between the 3-month LIBOR and Repo rate on 3-month UK gilts. Since we have a new dataset of indicators, we first difference for stationarity and perform PCA, similarly to our preliminary analysis on Kiley's dataset:

```{r echo = FALSE, warning = FALSE, message = FALSE}
aikman = read_csv("aikman_labeled.csv") %>%
  select_at(-c(1:3)) %>%
  mutate_all(as.numeric)

aikman_stationary = data.frame(sapply(aikman, diff)) %>%
  select_at(-1) %>%
  mutate_all(scale)

aikmanPCA = prcomp(aikman_stationary)
dfPCA = as_tibble(aikmanPCA$rotation) %>%
  mutate(feature = rownames(aikmanPCA$rotation)) %>%
  arrange(-PC1) %>%
  select_at(c(1:3, 18)) %>%
  pivot_longer(1:3, names_to = 'component', values_to = 'loading')

# heatmap of loadings
ggplot(dfPCA) +
  geom_tile(aes(x = feature, y = component, fill = loading)) +
  scale_fill_distiller(palette = "RdBu") +
  coord_equal() +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 9))

# first 2 PCs by log libor-repo
outcome_LR = log(aikman$outcome_LiborRepo[2:90])
df_outcome = cbind(aikmanPCA$x, outcome_LR)
ggplot(df_outcome) +
  geom_point(aes(x = PC1, y = PC2, color = outcome_LR)) +
  scale_color_viridis_c() +
  labs(x = "PC1 (25.55%)", y = "PC2 (15.76%)") +
  theme_minimal()
```

Looks OK so far --  we see one clear outlier in the scatterplot, but see slightly more variance explained by the first two PCs, a good sign. The heatmap looks reasonable (credit and certain asset pricing variables have the most influence on PC1, and nonfinancial sector variables on the other two). Let's now fit the following model, essentially a repeat of the Kiley-TED spread specification:

$$
\log(r_t) = \alpha + \theta_1 \log(r_{t-1}) + \Omega \boldsymbol{A_{t-1}} + \epsilon_t
$$
where we're modeling the log of the LIBOR-Repo spread $r_t$ in period $t$ given the log LR spread in period $t-1$ and a vector of differenced indicators $\boldsymbol{X_{t-1}}$ reflecting the change in each series in Aikman's dataset from period $t-2$ to period $t-1$. 

```{r echo = FALSE, warning = FALSE}
LR = log(aikman$outcome_LiborRepo)
.lagLR = LR[2:71]
x2 = data.matrix(cbind(aikman_stationary[1:70,], .lagLR))
y2 = LR[3:72]
x2_test = data.matrix(cbind(aikman_stationary[71:88,], LR[72:89]))
y2_test = LR[73:90]

model_arima = auto.arima(outcome_LR)
pred_ar = predict(model_arima, n.ahead = length(y2_test))
mae_ar = mean(abs(y2_test - pred_ar$pred))

seq_lambda = seq(0.2, 0, by = -0.001)
mae_train = vector(mode = 'numeric', length = 200)
mae_test = vector(mode = 'numeric', length = 200)
lasso1 = glmnet(x2, y2, type.measure = "mse", alpha = 1, lambda = seq_lambda)
for (lambda in seq_lambda) {
  pred_train = predict(lasso1, s = lambda, newx = x2)
  pred_test = predict(lasso1, s = lambda, newx = x2_test)
  mae_train[which(seq_lambda == lambda)] = mean(abs(pred_train - y2))
  mae_test[which(seq_lambda == lambda)] = mean(abs(pred_test - y2_test))
}

### produce model results
# train and test error per lambda
df = data.frame('lambda' = seq_lambda, mae_train, mae_test) %>%
  pivot_longer(2:3, names_to = "series", values_to = "mae")
ggplot(df) +
  geom_line(aes(x = lambda, y = mae, color = series)) +
  theme_minimal()

# r2 and pct improvement over ar(1)
df_coef = enframe(data.matrix(coef(lasso1))[,which.min(mae_test)]) %>%
  arrange(-abs(value)) %>%
  mutate(value = round(value, digits = 3)) %>%
  rename(Coefficient = value) %>%
  separate(name, into = c("Class", "Indicator"), fill = "left")

# predictions
df_pred = data.frame(predict(lasso1, s = seq_lambda[which.min(mae_test)], newx = x2_test),
                     as.double(pred_ar$pred), y2_test) %>%
  mutate(time = 1:length(y2_test))
colnames(df_pred) = c('lasso', 'AR_1', 'LR', 'time')
df_pred = df_pred %>%
  pivot_longer(1:3, names_to = 'series', values_to = 'value')
ggplot(df_pred) +
  geom_line(aes(x = time, y = value, color = series)) +
  theme_minimal()
```

```{r echo = FALSE, results = 'asis'}
stargazer(df_coef, summary = FALSE, rownames = FALSE, header = FALSE)
```

```{r}
(r2 = lasso1$dev.ratio[which.min(mae_test)])
(pct_improvement_log = (mae_ar - min(mae_test)) / mae_ar)
```

The model doesn't seem to converge on this smaller dataset, even when we compensate by regularizing more severely (increasing $\lambda$ beyond 0.1). TBD how to fix this. The other diagnostics look OK though, but I think the $R^2$ value is an artifact of overfitting (look how much lower the training error is than the test error), and the reasonably good percent improvement might just show that the LR spread is more of a random-walk type thing that can't be modeled accurately by time series forecasting. 

### Next Steps

I personally think the above is a reasonably good start, but more needs to be done to turn this into an actual good thesis. Some things to explore next might be more advanced time series modeling on the outcome variables themselves (if we buy that there is conditional heteroskedasticity in the outcome variables, which it looks like there might be for the FSS index, GARCH or SV models might be interesting to look at). My thinking here is that generating the FSS scores was itself nontrivial, and we might want to feature that work in a slightly more significant way rather than just using it as another data source as if I just downloaded it from Bloomberg or something. Something else I'd like to try, but with no guarantees at all that it would work, are deep learning methods like LSTM neural networks. I don't think we have enough data to do this successfully, but would be interesting to try it regardless. Beyond that, I think coming up with a better validation/identification strategy that gives a little more information than just a percentage improvement over a baseline time series model on the test set, is key to understanding what's actually going on in these datasets. Another thing to consider is the noise within the FSS index, shown here:

```{r echo = FALSE, warning = FALSE}
kiley = read_excel("kiley_labeled_FSS.xlsx") %>%
  select_at(3:29) %>%
  mutate_all(as.numeric) 

ggplot(kiley) +
  geom_line(aes(x = seq(1990.25,2019.5,by = 0.25), y = scale(outcome_FSS))) +
  labs(x = "time", y = "outcome_FSS [standardized]") +
  theme_minimal()
```

We really need some way to denoise this signal so we get a nice smooth line, maybe with the peaks we see near the dot-com boom and the Recession. This might just not be what the data is showing us, which would be disappointing, but not sure what else we can do.

